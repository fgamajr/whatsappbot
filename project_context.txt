==================================================
           ESTRUTURA DO PROJETO (tree -L 3)       
==================================================

.
├── .dockerignore
├── .env.example
├── .gitignore
├── README.md
├── app
│   ├── __init__.py
│   ├── api
│   │   ├── __init__.py
│   │   ├── middleware
│   │   └── v1
│   ├── core
│   │   ├── __init__.py
│   │   ├── config.py
│   │   ├── exceptions.py
│   │   └── logging.py
│   ├── domain
│   │   ├── __init__.py
│   │   ├── entities
│   │   └── value_objects
│   ├── infrastructure
│   │   ├── __init__.py
│   │   ├── ai
│   │   ├── database
│   │   └── whatsapp
│   ├── main.py
│   ├── prompts
│   │   ├── __init__.py
│   │   └── interview_analysis.py
│   ├── services
│   │   ├── __init__.py
│   │   ├── analysis.py
│   │   ├── audio_processor.py
│   │   ├── document_generator.py
│   │   ├── message_handler.py
│   │   └── transcription.py
│   └── utils
│       └── __init__.py
├── docker
│   └── Dockerfile
├── docker-compose.yml
├── fly.toml
├── gerar.sh
├── logs
├── project_context.txt
├── requirements.txt
├── scripts
│   ├── run.sh
│   ├── setup.sh
│   └── test.sh
└── tests
    ├── __init__.py
    ├── e2e
    │   └── __init__.py
    ├── integration
    │   └── __init__.py
    ├── test_main.py
    └── unit
        ├── __init__.py
        └── test_phone_number.py

27 directories, 33 files


==================================================
       CÓDIGO FONTE E ARQUIVOS DE CONFIGURAÇÃO    
==================================================

# --------------------------------------------------
# Arquivo: ./README.md
# --------------------------------------------------
# 🎙️ WhatsApp Interview Bot - Enterprise Edition

Sistema profissional de transcrição e análise de entrevistas via WhatsApp com arquitetura limpa e processamento em background.

## 🚀 Características

- **Arquitetura Limpa**: Separação clara de responsabilidades (Clean Architecture + DDD)
- **Processamento Background**: Resposta imediata (<1s) + processamento assíncrono
- **AI Stack Especializada**: Whisper (transcrição) + Gemini (análise)
- **MongoDB Atlas**: Database cloud gerenciado com backup automático
- **Production Ready**: Docker, health checks, monitoring, logs estruturados

## 🏗️ Arquitetura

```
app/
├── main.py                    # FastAPI setup (50 linhas!)
├── api/v1/                   # Controllers
├── domain/                   # Entidades e regras de negócio
├── services/                 # Lógica de aplicação
├── infrastructure/           # Integrações externas
├── core/                     # Configuração e utilitários
└── prompts/                  # Prompts de IA
```

## 🛠️ Setup Rápido

### 1. Instalação
```bash
# Executar o script de setup
./scripts/setup.sh

# Editar variáveis de ambiente
cp .env.example .env
# Edite o .env com suas credenciais
```

### 2. Configuração Obrigatória

```bash
# .env
WHATSAPP_TOKEN=your_token
WHATSAPP_VERIFY_TOKEN=your_verify_token
PHONE_NUMBER_ID=your_phone_id
OPENAI_API_KEY=your_openai_key
GEMINI_API_KEY=your_gemini_key
MONGODB_URL=mongodb+srv://...
```

### 3. Execução

```bash
# Desenvolvimento
./scripts/run.sh dev

# Produção
./scripts/run.sh

# Docker
docker-compose up -d
```

## 📦 Funcionalidades

### 🎵 Processamento de Áudio
- **Suporte**: Qualquer duração de áudio
- **Chunks**: Divisão automática em segmentos de 15min
- **Formatos**: Conversão automática para MP3
- **Progress**: Updates em tempo real

### 🎙️ Transcrição (Whisper)
- **Timestamps**: Precisão em milissegundos
- **Idioma**: Português otimizado
- **Modos**: 
  - Completo (com identificação de locutores)
  - Simples (apenas timestamps)

### 🧠 Análise (Gemini)
- **Avaliação Profissional**: Experiência e conquistas
- **Perfil Pessoal**: Motivações e valores
- **Análise Comportamental**: Soft skills e liderança
- **Recomendações**: Pontos fortes e desenvolvimento

### 📄 Documentos
- **Transcrição**: DOCX com timestamps formatados
- **Análise**: DOCX estruturado com insights
- **Entrega**: Via WhatsApp automaticamente

## 🔧 Comandos do Bot

| Comando | Função |
|---------|--------|
| `help` | Manual completo |
| `status` | Status do sistema |
| `/completo` | Modo com locutores |
| `/simples` | Modo sem locutores |

## 🏥 Monitoramento

### Health Checks
```bash
# Liveness
curl http://localhost:8000/health/live

# Readiness (com dependências)
curl http://localhost:8000/health/ready
```

### Logs Estruturados
```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "level": "INFO",
  "message": "Interview processing started",
  "interview_id": "abc123",
  "phone_number": "5511999887766"
}
```

## 🚀 Deploy Produção

### Docker
```bash
# Build
docker build -f docker/Dockerfile -t interview-bot .

# Run
docker-compose up -d
```

### Kubernetes (opcional)
```bash
# TODO: Adicionar manifests K8s
kubectl apply -f k8s/
```

## 🧪 Testes

```bash
# Executar todos os testes
./scripts/test.sh

# Apenas testes unitários
pytest tests/unit/

# Com coverage
pytest --cov=app tests/
```

## 📊 Performance

| Métrica | Valor |
|---------|-------|
| Webhook Response | <1s |
| Audio 15min | ~3-5min |
| Audio 60min | ~12-20min |
| Concurrent Users | 50+ |
| Uptime Target | 99.9% |

## 🔒 Segurança

- **Validação**: Input sanitization
- **Rate Limiting**: Anti-spam
- **Secrets**: Environment variables
- **HTTPS**: TLS obrigatório
- **Logs**: Sem dados sensíveis

## 📈 Escalabilidade

- **Horizontal**: Load balancer + múltiplas instâncias
- **Database**: MongoDB Atlas auto-scaling
- **Cache**: Redis (opcional)
- **Queue**: Background tasks assíncronas

## 🐛 Troubleshooting

### Problemas Comuns

1. **Webhook não responde**
   ```bash
   # Verificar logs
   docker logs interview-bot
   
   # Verificar health
   curl http://localhost:8000/health/ready
   ```

2. **Transcrição falha**
   ```bash
   # Verificar quota OpenAI
   # Verificar formato do áudio
   # Verificar logs do Whisper
   ```

3. **MongoDB connection**
   ```bash
   # Verificar connection string
   # Verificar IP whitelist no Atlas
   ```

## 🤝 Contribuição

1. Fork o projeto
2. Crie uma branch: `git checkout -b feature/nova-funcionalidade`
3. Commit: `git commit -m 'Adiciona nova funcionalidade'`
4. Push: `git push origin feature/nova-funcionalidade`
5. Pull Request

## 📄 Licença

MIT License - veja [LICENSE](LICENSE) para detalhes.

## 🔗 Links Úteis

- [WhatsApp Business API](https://developers.facebook.com/docs/whatsapp)
- [OpenAI Whisper](https://openai.com/research/whisper)
- [Google Gemini](https://deepmind.google/technologies/gemini/)
- [MongoDB Atlas](https://www.mongodb.com/atlas)
- [FastAPI Docs](https://fastapi.tiangolo.com/)

---

🚀 **Feito com Clean Architecture para produção enterprise!**

# --------------------------------------------------
# Arquivo: ./app/api/middleware/error_handler.py
# --------------------------------------------------
from fastapi import Request, Response
from starlette.middleware.base import BaseHTTPMiddleware
import logging
import json
from app.core.exceptions import InterviewBotException

logger = logging.getLogger(__name__)


class ErrorHandlerMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        try:
            response = await call_next(request)
            return response
        
        except InterviewBotException as e:
            logger.error("Application error", extra={
                "error_type": type(e).__name__,
                "error_code": e.error_code,
                "message": e.message,
                "path": str(request.url)
            })
            
            return Response(
                content=json.dumps({
                    "error": e.message,
                    "error_code": e.error_code
                }),
                status_code=400,
                media_type="application/json"
            )
        
        except Exception as e:
            logger.error("Unexpected error", extra={
                "error": str(e),
                "path": str(request.url)
            })
            
            return Response(
                content=json.dumps({
                    "error": "Internal server error"
                }),
                status_code=500,
                media_type="application/json"
            )

# --------------------------------------------------
# Arquivo: ./app/api/v1/health.py
# --------------------------------------------------
from fastapi import APIRouter
from typing import Dict
import logging
from app.infrastructure.database.mongodb import MongoDB
from app.core.config import settings

logger = logging.getLogger(__name__)
router = APIRouter()


@router.get("/live")
async def liveness():
    """Liveness probe"""
    return {"status": "alive", "service": "interview-bot"}


@router.get("/ready")
async def readiness():
    """Readiness probe with dependencies check"""
    health_status = {
        "status": "healthy",
        "version": settings.VERSION,
        "environment": settings.ENVIRONMENT,
        "services": {}
    }
    
    # Check MongoDB
    try:
        db = await MongoDB.get_database()
        await db.command("ping")
        health_status["services"]["mongodb"] = "connected"
    except Exception as e:
        health_status["services"]["mongodb"] = f"error: {str(e)}"
        health_status["status"] = "unhealthy"
    
    # Check AI services configuration
    health_status["services"]["openai"] = "configured" if settings.OPENAI_API_KEY else "missing_key"
    health_status["services"]["gemini"] = "configured" if settings.GEMINI_API_KEY else "missing_key"
    health_status["services"]["whatsapp"] = "configured" if settings.WHATSAPP_TOKEN else "missing_token"
    
    status_code = 200 if health_status["status"] == "healthy" else 503
    return health_status

# --------------------------------------------------
# Arquivo: ./app/api/v1/webhooks.py
# --------------------------------------------------
from fastapi import APIRouter, Request, Response, BackgroundTasks
from typing import Dict, Set
import logging
from app.services.message_handler import MessageHandler
from app.domain.value_objects.phone_number import BrazilianPhoneNumber
from app.core.config import settings

logger = logging.getLogger(__name__)
router = APIRouter()

# Simple in-memory cache for duplicate detection
processed_messages: Set[str] = set()


@router.get("")
async def verify_webhook(request: Request):
    """Verify webhook with WhatsApp"""
    mode = request.query_params.get("hub.mode")
    token = request.query_params.get("hub.verify_token")
    challenge = request.query_params.get("hub.challenge")

    if mode == "subscribe" and token == settings.WHATSAPP_VERIFY_TOKEN:
        logger.info("Webhook verified successfully")
        return Response(content=challenge, status_code=200)
    else:
        logger.warning("Webhook verification failed")
        return Response(status_code=403)


@router.post("")
async def whatsapp_webhook(request: Request, background_tasks: BackgroundTasks):
    """Main webhook endpoint - immediate response with background processing"""
    try:
        data = await request.json()
        
        # Quick validation
        if not _is_valid_message(data):
            return Response(status_code=200)
        
        # Extract message data
        message_data = _extract_message_data(data)
        if not message_data:
            return Response(status_code=200)
        
        # Handle different message types
        if message_data["type"] == "audio":
            # Schedule background processing
            handler = MessageHandler()
            background_tasks.add_task(handler.process_audio_message, message_data)
            
            logger.info("Audio processing scheduled", extra={
                "message_id": message_data["message_id"],
                "from": message_data["from"]
            })
        
        elif message_data["type"] == "text":
            # Handle text commands immediately
            await _handle_text_message(message_data)
        
        return Response(status_code=200)
        
    except Exception as e:
        logger.error("Webhook processing error", extra={
            "error": str(e)
        })
        return Response(status_code=500)


def _is_valid_message(data: dict) -> bool:
    """Check if webhook contains valid message"""
    try:
        entry = data.get("entry", [])
        if not entry:
            return False
            
        changes = entry[0].get("changes", [])
        if not changes:
            return False
            
        value = changes[0].get("value", {})
        
        # Ignore status updates
        if "statuses" in value:
            return False
        
        # Check for messages
        messages = value.get("messages", [])
        return bool(messages)
        
    except (IndexError, KeyError):
        return False


def _extract_message_data(data: dict) -> Dict:
    """Extract message data with duplicate protection"""
    try:
        messages = data["entry"][0]["changes"][0]["value"]["messages"][0]
        
        message_id = messages["id"]
        message_type = messages["type"]
        from_number = messages["from"]
        
        # Check for duplicates
        if message_id in processed_messages:
            logger.info("Duplicate message ignored", extra={
                "message_id": message_id
            })
            return None
        
        # Add to cache
        processed_messages.add(message_id)
        
        # Clean cache if too large
        if len(processed_messages) > settings.MAX_CACHE_SIZE:
            processed_messages.clear()
        
        # Validate and fix phone number
        try:
            phone = BrazilianPhoneNumber(number=from_number)
            from_number = phone.number
        except:
            pass  # Use original number if validation fails
        
        result = {
            "from": from_number,
            "type": message_type,
            "message_id": message_id,
            "timestamp": messages.get("timestamp")
        }
        
        if message_type == "audio":
            result["media_id"] = messages["audio"]["id"]
        elif message_type == "text":
            result["content"] = messages["text"]["body"]
        
        return result
        
    except (KeyError, IndexError) as e:
        logger.error("Error extracting message data", extra={
            "error": str(e)
        })
        return None


async def _handle_text_message(message_data: Dict):
    """Handle text commands immediately"""
    from app.infrastructure.whatsapp.client import WhatsAppClient
    
    whatsapp = WhatsAppClient()
    from_number = message_data["from"]
    text = message_data["content"].lower().strip()
    
    if text in ["help", "ajuda", "/help"]:
        help_message = """
📋 *Bot de Relatório de Entrevistas* - Sistema Enterprise

🎵 **Processamento em Background:**
• Resposta imediata ao WhatsApp (<1s)
• Processamento paralelo de áudios longos
• Chunks otimizados de 15min
• Progress updates em tempo real
• Arquitetura limpa e escalável

📄 **Você receberá 2 documentos:**
1️⃣ **TRANSCRIÇÃO** - Texto completo com timestamps precisos
2️⃣ **ANÁLISE** - Relatório estruturado profissional

🎙️ **Modos de Transcrição:**
• **Padrão:** Com identificação de locutores (ENTREVISTADOR/CANDIDATO)
• **`/simples`:** Sem identificação de locutores (apenas timestamps)

🚀 **Como usar:**
Apenas envie o áudio da entrevista (QUALQUER duração)!

💡 **Comandos úteis:**
• `help` - Esta mensagem
• `status` - Informações do sistema
• `/simples` - Ativar modo sem locutores
• `/completo` - Ativar modo com locutores
        """
        await whatsapp.send_text_message(from_number, help_message)
    
    elif text == "status":
        status_message = f"""
📊 *System Status*

⚡ **Mode:** Background processing enabled
🚀 **Architecture:** Clean & Scalable
💾 **Cache:** {len(processed_messages)} messages processed
🛡️ **Protection:** Anti-duplicate enabled
🎙️ **Transcription:** Whisper + Gemini
🗄️ **Database:** MongoDB Atlas

🔄 **Commands:**
• `/completo` - Modo com locutores
• `/simples` - Modo sem locutores
        """
        await whatsapp.send_text_message(from_number, status_message)
    
    else:
        await whatsapp.send_text_message(
            from_number, 
            "👋 Envie-me uma gravação de áudio de entrevista!\n⚡ Resposta imediata + processamento enterprise em background!"
        )

# --------------------------------------------------
# Arquivo: ./app/core/config.py
# --------------------------------------------------
from functools import lru_cache
from typing import Optional

# Imports corretos para Pydantic V2
from pydantic import ConfigDict
from pydantic_settings import BaseSettings


class Settings(BaseSettings):
    # Estrutura correta, usando model_config e sem a 'class Config'
    model_config = ConfigDict(env_file=".env", case_sensitive=True)

    # App
    APP_NAME: str = "Interview Bot"
    VERSION: str = "2.0.0"
    DEBUG: bool = False
    ENVIRONMENT: str = "development"
    
    # WhatsApp
    WHATSAPP_TOKEN: str
    WHATSAPP_VERIFY_TOKEN: str
    PHONE_NUMBER_ID: str
    WHATSAPP_API_VERSION: str = "v18.0"
    
    # AI Services
    OPENAI_API_KEY: str
    GEMINI_API_KEY: str
    WHISPER_MODEL: str = "whisper-1"
    
    # Database
    MONGODB_URL: str
    DB_NAME: str = "interview_bot"
    
    # Processing
    AUDIO_CHUNK_MINUTES: int = 15
    MAX_RETRIES: int = 3
    MAX_CACHE_SIZE: int = 1000
    
    # Rate Limiting
    RATE_LIMIT_PER_MINUTE: int = 10
    RATE_LIMIT_PER_HOUR: int = 100


@lru_cache()
def get_settings():
    return Settings()


settings = get_settings()
# --------------------------------------------------
# Arquivo: ./app/core/exceptions.py
# --------------------------------------------------
"""Custom exceptions for the application"""


class InterviewBotException(Exception):
    """Base exception for the application"""
    def __init__(self, message: str, error_code: str = None):
        self.message = message
        self.error_code = error_code
        super().__init__(self.message)


class AudioProcessingError(InterviewBotException):
    """Error during audio processing"""
    pass


class TranscriptionError(InterviewBotException):
    """Error during transcription"""
    pass


class AnalysisError(InterviewBotException):
    """Error during analysis generation"""
    pass


class WhatsAppError(InterviewBotException):
    """Error with WhatsApp API"""
    pass


class DatabaseError(InterviewBotException):
    """Error with database operations"""
    pass


class ConfigurationError(InterviewBotException):
    """Error with configuration"""
    pass

# --------------------------------------------------
# Arquivo: ./app/core/logging.py
# --------------------------------------------------
import logging
import sys
from typing import Dict, Any
import json
from datetime import datetime


class StructuredFormatter(logging.Formatter):
    """Custom formatter for structured logging"""
    
    def format(self, record: logging.LogRecord) -> str:
        log_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
        }
        
        # Add extra fields if present
        if hasattr(record, "extra"):
            log_data.update(record.extra)
            
        return json.dumps(log_data, ensure_ascii=False)


def setup_logging(debug: bool = False) -> None:
    """Setup structured logging"""
    level = logging.DEBUG if debug else logging.INFO
    
    # Remove default handlers
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    # Create structured handler
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(StructuredFormatter())
    
    # Configure root logger
    logging.basicConfig(
        level=level,
        handlers=[handler],
        force=True
    )
    
    # Set specific loggers
    logging.getLogger("uvicorn").setLevel(logging.INFO)
    logging.getLogger("fastapi").setLevel(logging.INFO)
    
    logger = logging.getLogger(__name__)
    logger.info("Structured logging configured", extra={
        "debug_mode": debug,
        "level": level
    })

# --------------------------------------------------
# Arquivo: ./app/domain/entities/audio.py
# --------------------------------------------------
from typing import List, Tuple
from pydantic import BaseModel


class AudioChunk(BaseModel):
    index: int
    start_time_minutes: float
    duration_minutes: float
    size_bytes: int
    
    
class AudioFile(BaseModel):
    media_id: str
    size_mb: float
    duration_minutes: Optional[float] = None
    format: str = "audio/ogg"
    chunks: List[AudioChunk] = []
    
    def add_chunk(self, chunk: AudioChunk):
        self.chunks.append(chunk)
    
    @property
    def total_chunks(self) -> int:
        return len(self.chunks)

# --------------------------------------------------
# Arquivo: ./app/domain/entities/interview.py
# --------------------------------------------------
from datetime import datetime
from typing import Optional, List
from pydantic import BaseModel, Field
from enum import Enum
from pydantic import BaseModel, Field, ConfigDict


class InterviewStatus(str, Enum):
    PENDING = "pending"
    PROCESSING = "processing"
    TRANSCRIBING = "transcribing"
    ANALYZING = "analyzing"
    COMPLETED = "completed"
    FAILED = "failed"



class Interview(BaseModel):
    model_config = ConfigDict(use_enum_values=True)
    id: str = Field(default_factory=lambda: str(int(datetime.now().timestamp() * 1000)))
    phone_number: str
    message_id: str
    status: InterviewStatus = InterviewStatus.PENDING
    
    # Audio info
    audio_id: str
    audio_size_mb: float = 0.0
    duration_minutes: Optional[float] = None
    
    # Processing info
    chunks_total: int = 0
    chunks_processed: int = 0
    
    # Results
    transcript: Optional[str] = None
    analysis: Optional[str] = None
    
    # Files
    transcript_file_id: Optional[str] = None
    analysis_file_id: Optional[str] = None
    
    # Metadata
    created_at: datetime = Field(default_factory=datetime.now)
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    error: Optional[str] = None
    
    # User preferences
    simple_mode: bool = False
    

        
    def mark_processing(self):
        self.status = InterviewStatus.PROCESSING
        self.started_at = datetime.now()
    
    def mark_completed(self):
        self.status = InterviewStatus.COMPLETED
        self.completed_at = datetime.now()
    
    def mark_failed(self, error: str):
        self.status = InterviewStatus.FAILED
        self.error = error
        self.completed_at = datetime.now()

# --------------------------------------------------
# Arquivo: ./app/domain/value_objects/phone_number.py
# --------------------------------------------------
from pydantic import BaseModel, field_validator, validator
import re


class BrazilianPhoneNumber(BaseModel):
    number: str
    
    @field_validator('number')
    @classmethod
    def validate_brazilian_number(cls, v):
        # Remove any non-digits
        clean_number = re.sub(r'\D', '', v)
        
        # Brazilian mobile pattern: 55 + area code (2 digits) + mobile number
        if not clean_number.startswith('55'):
            raise ValueError('Number must start with country code 55')
        
        # Check length (should be 13 digits for mobile)
        if len(clean_number) not in [12, 13]:
            raise ValueError('Invalid Brazilian mobile number length')
        
        # Fix missing 9th digit if needed
        if len(clean_number) == 12:
            area_code = clean_number[2:4]
            valid_area_codes = [
                "11", "12", "13", "14", "15", "16", "17", "18", "19",
                "21", "22", "24", "27", "28", "31", "32", "33", "34",
                "35", "37", "38", "41", "42", "43", "44", "45", "46",
                "47", "48", "49", "51", "53", "54", "55", "61", "62",
                "63", "64", "65", "66", "67", "68", "69", "71", "73",
                "74", "75", "77", "79", "81", "82", "83", "84", "85",
                "86", "87", "88", "89", "91", "92", "93", "94", "95",
                "96", "97", "98", "99"
            ]
            
            if area_code in valid_area_codes:
                # Insert 9 after area code
                clean_number = clean_number[:4] + "9" + clean_number[4:]
        
        return clean_number
    
    def __str__(self):
        return self.number

# --------------------------------------------------
# Arquivo: ./app/infrastructure/ai/gemini.py
# --------------------------------------------------
import google.generativeai as genai
from typing import Optional
import logging
from app.core.config import settings
from app.core.exceptions import AnalysisError

logger = logging.getLogger(__name__)


class GeminiService:
    def __init__(self):
        genai.configure(api_key=settings.GEMINI_API_KEY)
        self.model = genai.GenerativeModel('models/gemini-1.5-pro')
        
    async def generate_analysis(self, transcript: str, prompt: str) -> Optional[str]:
        """Generate analysis using Gemini"""
        try:
            final_prompt = f"""TRANSCRIPT:
{transcript}

INSTRUCTIONS:
{prompt}"""
            
            logger.info("Starting Gemini analysis", extra={
                "transcript_length": len(transcript),
                "prompt_length": len(prompt)
            })
            
            response = self.model.generate_content(final_prompt)
            
            if response and response.text:
                logger.info("Gemini analysis completed", extra={
                    "response_length": len(response.text)
                })
                return response.text.strip()
            else:
                logger.warning("Gemini returned empty response")
                return None
                
        except Exception as e:
            logger.error("Gemini analysis failed", extra={
                "error": str(e)
            })
            raise AnalysisError(f"Failed to generate analysis: {str(e)}")

# --------------------------------------------------
# Arquivo: ./app/infrastructure/ai/whisper.py
# --------------------------------------------------
import openai
import httpx
from typing import Optional, Dict
import logging
import io
import traceback  # Import for detailed error printing
from app.core.config import settings
from app.core.exceptions import TranscriptionError

logger = logging.getLogger(__name__)


class WhisperService:
    """
    Service to interact with the OpenAI Whisper API for audio transcription.
    """
    def __init__(self):
        """
        Initializes the asynchronous OpenAI client.
        
        An explicit httpx.AsyncClient is passed to avoid potential issues
        with proxy configurations that the default client might pick up.
        """
        self.client = openai.AsyncOpenAI(
            api_key=settings.OPENAI_API_KEY,
            http_client=httpx.AsyncClient()
        )
        
    async def transcribe(
        self, 
        audio_bytes: bytes,
        language: str = "pt",
        response_format: str = "verbose_json"
    ) -> Dict:
        """
        Transcribes an audio file using the Whisper API.

        This function sends the audio bytes to OpenAI and returns a structured
        dictionary with the full transcript and timestamped segments.

        Args:
            audio_bytes: The audio content in bytes.
            language: The language of the audio (ISO 639-1 format).
            response_format: The desired format for the response. 'verbose_json'
                             provides detailed segments and timestamps.

        Returns:
            A dictionary containing the transcription text and segments.
        
        Raises:
            TranscriptionError: If the transcription fails at any stage.
        """
        try:
            # The OpenAI API requires a file-like object with a name.
            audio_file = io.BytesIO(audio_bytes)
            audio_file.name = "audio.mp3"

            logger.info("Starting Whisper transcription", extra={
                "audio_size_bytes": len(audio_bytes),
                "language": language
            })

            # Create the transcription request
            response = await self.client.audio.transcriptions.create(
                model=settings.WHISPER_MODEL, # Using model from config for flexibility
                file=audio_file,
                language=language,
                response_format=response_format,
            )

            # Structure the result
            result = {
                "text": response.text,
                "segments": getattr(response, 'segments', [])
            }

            logger.info("Whisper transcription completed successfully", extra={
                "text_length": len(result["text"]),
                "segments_count": len(result["segments"])
            })

            return result

        # Catches specific API errors from OpenAI (e.g., invalid key, no credits)
        except openai.APIStatusError as e:
            # --- Detailed tracing for API errors is active ---
            print("\n\n================================================")
            print(">>> WHISPER API ERROR (e.g., auth, billing) <<<")
            traceback.print_exc()
            print("================================================\n\n")
            
            logger.error(
                "Whisper transcription failed due to OpenAI API error",
                extra={
                    "status_code": e.status_code,
                    "error_message": str(e),
                    "response_body": e.body, 
                }
            )
            raise TranscriptionError(f"OpenAI API Error: {str(e)}")

        # Catches any other unexpected errors (e.g., network issues)
        except Exception as e:
            # --- Detailed tracing for unexpected errors is active ---
            print("\n\n================================================")
            print(">>> UNEXPECTED WHISPER ERROR (e.g., network) <<<")
            traceback.print_exc()
            print("================================================\n\n")

            logger.error(
                "Whisper transcription failed due to an unexpected error",
                extra={
                    "error_type": type(e).__name__,
                    "error_message": str(e),
                }
            )
            raise TranscriptionError(f"Unexpected error during transcription: {str(e)}")


# --------------------------------------------------
# Arquivo: ./app/infrastructure/database/mongodb.py
# --------------------------------------------------
from motor.motor_asyncio import AsyncIOMotorClient, AsyncIOMotorDatabase
from app.core.config import settings
import logging

logger = logging.getLogger(__name__)


class MongoDB:
    client: AsyncIOMotorClient = None
    database: AsyncIOMotorDatabase = None
    
    @classmethod
    async def connect(cls):
        """Create database connection"""
        try:
            cls.client = AsyncIOMotorClient(
                settings.MONGODB_URL,
                maxPoolSize=50,
                minPoolSize=10,
                serverSelectionTimeoutMS=5000,
            )
            
            # Test connection
            await cls.client.admin.command('ping')
            
            cls.database = cls.client[settings.DB_NAME]
            
            logger.info("MongoDB connected successfully", extra={
                "database": settings.DB_NAME,
                "url": settings.MONGODB_URL.split('@')[-1]  # Hide credentials
            })
            
        except Exception as e:
            logger.error("Failed to connect to MongoDB", extra={
                "error": str(e)
            })
            raise
    
    @classmethod
    async def disconnect(cls):
        """Close database connection"""
        if cls.client:
            cls.client.close()
            logger.info("MongoDB disconnected")
    
    @classmethod
    async def get_database(cls) -> AsyncIOMotorDatabase:
        """Get database instance"""
        if cls.database is None:
            await cls.connect()
        return cls.database

# --------------------------------------------------
# Arquivo: ./app/infrastructure/database/repositories/interview.py
# --------------------------------------------------
from typing import Optional, List
from motor.motor_asyncio import AsyncIOMotorCollection
from app.domain.entities.interview import Interview, InterviewStatus
from app.infrastructure.database.mongodb import MongoDB
from app.core.exceptions import DatabaseError
import logging

logger = logging.getLogger(__name__)


class InterviewRepository:
    def __init__(self):
        self.collection: AsyncIOMotorCollection = None
    
    async def _get_collection(self) -> AsyncIOMotorCollection:
        if self.collection is None:
            db = await MongoDB.get_database()
            self.collection = db.interviews
            
            # Create indexes
            await self.collection.create_index("phone_number")
            await self.collection.create_index("message_id", unique=True)
            await self.collection.create_index("created_at")
            await self.collection.create_index("status")
            
        return self.collection

    
    async def create(self, interview: Interview) -> Interview:
        try:
            collection = await self._get_collection()
            await collection.insert_one(interview.dict())
            
            logger.info("Interview created", extra={
                "interview_id": interview.id,
                "phone_number": interview.phone_number
            })
            
            return interview
            
        except Exception as e:
            logger.error("Failed to create interview", extra={
                "error": str(e),
                "interview_id": interview.id
            })
            raise DatabaseError(f"Failed to create interview: {str(e)}")
    
    async def get_by_id(self, interview_id: str) -> Optional[Interview]:
        try:
            collection = await self._get_collection()
            data = await collection.find_one({"id": interview_id})
            return Interview(**data) if data else None
            
        except Exception as e:
            logger.error("Failed to get interview by ID", extra={
                "error": str(e),
                "interview_id": interview_id
            })
            return None
    
    async def get_by_message_id(self, message_id: str) -> Optional[Interview]:
        try:
            collection = await self._get_collection()
            data = await collection.find_one({"message_id": message_id})
            return Interview(**data) if data else None
            
        except Exception as e:
            logger.error("Failed to get interview by message ID", extra={
                "error": str(e),
                "message_id": message_id
            })
            return None
    
    async def update(self, interview: Interview) -> Interview:
        try:
            collection = await self._get_collection()
            result = await collection.update_one(
                {"id": interview.id},
                {"$set": interview.dict()}
            )
            
            if result.matched_count == 0:
                raise DatabaseError(f"Interview not found: {interview.id}")
            
            logger.info("Interview updated", extra={
                "interview_id": interview.id,
                "status": interview.status
            })
            
            return interview
            
        except Exception as e:
            logger.error("Failed to update interview", extra={
                "error": str(e),
                "interview_id": interview.id
            })
            raise DatabaseError(f"Failed to update interview: {str(e)}")
    
    async def get_recent_by_phone(
        self, 
        phone_number: str, 
        limit: int = 10
    ) -> List[Interview]:
        try:
            collection = await self._get_collection()
            cursor = collection.find(
                {"phone_number": phone_number}
            ).sort("created_at", -1).limit(limit)
            
            interviews = []
            async for doc in cursor:
                interviews.append(Interview(**doc))
            
            return interviews
            
        except Exception as e:
            logger.error("Failed to get recent interviews", extra={
                "error": str(e),
                "phone_number": phone_number
            })
            return []
    
    async def get_processing_count(self) -> int:
        try:
            collection = await self._get_collection()
            return await collection.count_documents({
                "status": {"$in": [
                    InterviewStatus.PROCESSING,
                    InterviewStatus.TRANSCRIBING,
                    InterviewStatus.ANALYZING
                ]}
            })
            
        except Exception as e:
            logger.error("Failed to get processing count", extra={
                "error": str(e)
            })
            return 0

# --------------------------------------------------
# Arquivo: ./app/infrastructure/whatsapp/client.py
# --------------------------------------------------
import aiohttp
import os
from typing import Optional
import logging
import traceback
from app.core.config import settings
from app.core.exceptions import WhatsAppError

logger = logging.getLogger(__name__)


class WhatsAppClient:
    def __init__(self):
        self.token = settings.WHATSAPP_TOKEN
        self.phone_number_id = settings.PHONE_NUMBER_ID
        self.api_version = settings.WHATSAPP_API_VERSION
        self.base_url = f"https://graph.facebook.com/{self.api_version}"
        
    async def send_text_message(self, to_number: str, message: str) -> bool:
        """Send text message via WhatsApp"""
        try:
            url = f"{self.base_url}/{self.phone_number_id}/messages"
            headers = {
                "Authorization": f"Bearer {self.token}",
                "Content-Type": "application/json"
            }
            
            data = {
                "messaging_product": "whatsapp",
                "to": to_number,
                "type": "text",
                "text": {"body": message}
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=data, headers=headers) as response:
                    if response.status == 200:
                        logger.info("Text message sent", extra={
                            "to_number": to_number,
                            "message_length": len(message)
                        })
                        return True
                    else:
                        error_text = await response.text()
                        logger.error("Failed to send text message", extra={
                            "status": response.status,
                            "error": error_text,
                            "to_number": to_number
                        })
                        return False
                        
        except Exception as e:
            logger.error("Error sending text message", extra={
                "error": str(e),
                "to_number": to_number
            })
            traceback.print_exc()
            return False
    
    async def download_media(self, media_id: str) -> Optional[bytes]:
        """Download media file from WhatsApp"""
        try:
            # First, get the media URL
            url = f"{self.base_url}/{media_id}"
            headers = {"Authorization": f"Bearer {self.token}"}
            
            async with aiohttp.ClientSession() as session:
                # Get media URL
                async with session.get(url, headers=headers) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error("Failed to get media URL", extra={
                            "media_id": media_id,
                            "status": response.status,
                            "error": error_text
                        })
                        return None
                    
                    media_data = await response.json()
                    media_url = media_data.get("url")
                    
                    if not media_url:
                        logger.error("No media URL in response", extra={
                            "media_id": media_id,
                            "response_data": media_data
                        })
                        return None
                
                # Download the actual media file
                async with session.get(media_url, headers=headers) as response:
                    if response.status == 200:
                        content = await response.read()
                        logger.info("Media downloaded", extra={
                            "media_id": media_id,
                            "size_bytes": len(content)
                        })
                        return content
                    else:
                        error_text = await response.text()
                        logger.error("Failed to download media", extra={
                            "media_id": media_id,
                            "status": response.status,
                            "error": error_text
                        })
                        return None
                        
        except Exception as e:
            logger.error("Error downloading media", extra={
                "error": str(e),
                "media_id": media_id
            })
            traceback.print_exc()
            return None
    
    async def upload_media(self, file_path: str) -> Optional[str]:
        """Upload media file to WhatsApp using aiohttp"""
        try:
            url = f"{self.base_url}/{self.phone_number_id}/media"
            headers = {"Authorization": f"Bearer {self.token}"}
            
            # Verificar se o arquivo existe
            if not os.path.exists(file_path):
                logger.error("File not found", extra={"file_path": file_path})
                return None
            
            file_name = os.path.basename(file_path)
            
            # Determinar o MIME type baseado na extensão do arquivo
            mime_type_map = {
                '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
                '.pdf': 'application/pdf',
                '.txt': 'text/plain',
                '.jpg': 'image/jpeg',
                '.jpeg': 'image/jpeg',
                '.png': 'image/png',
                '.mp3': 'audio/mpeg',
                '.mp4': 'video/mp4',
                '.wav': 'audio/wav',
                '.ogg': 'audio/ogg'
            }
            
            file_extension = os.path.splitext(file_path)[1].lower()
            mime_type = mime_type_map.get(file_extension, 'application/octet-stream')
            
            # Usar aiohttp para upload assíncrono
            async with aiohttp.ClientSession() as session:
                # Criar o FormData para multipart/form-data
                data = aiohttp.FormData()
                
                # Adicionar campos de metadados
                data.add_field('messaging_product', 'whatsapp')
                data.add_field('type', 'document')
                
                # Adicionar o arquivo
                with open(file_path, 'rb') as f:
                    data.add_field('file', f, filename=file_name, content_type=mime_type)
                    
                    async with session.post(url, headers=headers, data=data) as response:
                        response_text = await response.text()
                        
                        if response.status == 200:
                            try:
                                response_json = await response.json()
                                media_id = response_json.get("id")
                                
                                if media_id:
                                    logger.info("Media uploaded successfully", extra={
                                        "file_path": file_path,
                                        "media_id": media_id,
                                        "file_size": os.path.getsize(file_path),
                                        "mime_type": mime_type
                                    })
                                    return media_id
                                else:
                                    logger.error("No media ID in response", extra={
                                        "file_path": file_path,
                                        "response": response_text
                                    })
                                    return None
                                    
                            except Exception as json_error:
                                logger.error("Failed to parse JSON response", extra={
                                    "file_path": file_path,
                                    "response": response_text,
                                    "json_error": str(json_error)
                                })
                                return None
                        else:
                            logger.error("Failed to upload media", extra={
                                "file_path": file_path,
                                "status": response.status,
                                "response": response_text
                            })
                            return None
                        
        except Exception as e:
            print("\n\n================================================")
            print(">>> ERRO INESPERADO DURANTE O UPLOAD PARA WHATSAPP <<<")
            traceback.print_exc()
            print("================================================\n\n")
            
            logger.error("Error uploading media", extra={
                "error": str(e),
                "file_path": file_path
            })
            return None
    
    async def send_document(self, to_number: str, media_id: str, caption: str, filename: str) -> bool:
        """Send document message via WhatsApp"""
        try:
            url = f"{self.base_url}/{self.phone_number_id}/messages"
            headers = {
                "Authorization": f"Bearer {self.token}",
                "Content-Type": "application/json"
            }
            
            data = {
                "messaging_product": "whatsapp",
                "to": to_number,
                "type": "document",
                "document": {
                    "id": media_id,
                    "caption": caption,
                    "filename": filename
                }
            }
            
            logger.info("Attempting to send document", extra={
                "to_number": to_number,
                "media_id": media_id,
                "document_filename": filename,
                "caption_length": len(caption) if caption else 0
            })
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=data, headers=headers) as response:
                    response_text = await response.text()
                    
                    if response.status == 200:
                        logger.info("Document sent successfully", extra={
                            "to_number": to_number,
                            "media_id": media_id,
                            "document_filename": filename
                        })
                        return True
                    else:
                        logger.error("Failed to send document", extra={
                            "status": response.status,
                            "error": response_text,
                            "to_number": to_number,
                            "media_id": media_id,
                            "request_data": data
                        })
                        return False
                        
        except Exception as e:
            print("\n\n================================================")
            print(">>> ERRO INESPERADO DURANTE O ENVIO DO DOCUMENTO <<<")
            traceback.print_exc()
            print("================================================\n\n")
            
            logger.error("Error sending document", extra={
                "error": str(e),
                "to_number": to_number,
                "media_id": media_id
            })
            return False
# --------------------------------------------------
# Arquivo: ./app/main.py
# --------------------------------------------------
from fastapi import FastAPI
from contextlib import asynccontextmanager
import logging

from app.api.v1 import webhooks, health
from app.api.middleware.error_handler import ErrorHandlerMiddleware
from app.core.config import settings
from app.core.logging import setup_logging
from app.infrastructure.database.mongodb import MongoDB

logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    setup_logging(debug=settings.DEBUG)
    logger.info("Starting Interview Bot", extra={
        "version": settings.VERSION,
        "environment": settings.ENVIRONMENT
    })
    
    await MongoDB.connect()
    
    yield
    
    # Shutdown
    await MongoDB.disconnect()
    logger.info("Interview Bot shutdown complete")


app = FastAPI(
    title=settings.APP_NAME,
    version=settings.VERSION,
    description="WhatsApp Interview Bot with Clean Architecture",
    lifespan=lifespan
)

# Middleware
app.add_middleware(ErrorHandlerMiddleware)

# Routes
app.include_router(health.router, prefix="/health", tags=["health"])
app.include_router(webhooks.router, prefix="/webhook", tags=["whatsapp"])


@app.get("/")
async def root():
    return {
        "message": f"{settings.APP_NAME} is running!",
        "version": settings.VERSION,
        "status": "healthy",
        "features": [
            "Clean Architecture",
            "Background Processing",
            "MongoDB Atlas",
            "Whisper + Gemini",
            "Production Ready"
        ]
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=8000,
        reload=settings.DEBUG
    )

# --------------------------------------------------
# Arquivo: ./app/prompts/interview_analysis.py
# --------------------------------------------------
INTERVIEW_ANALYSIS_PROMPT = """
Com base APENAS na transcrição da entrevista fornecida (que inclui identificação de ENTREVISTADOR e CANDIDATO), gere um relatório abrangente que responda às seguintes questões.
Estruture a saída com títulos claros para cada questão. Seja detalhado, mas conciso. Responda em PORTUGUÊS.

APROVEITE a separação entre ENTREVISTADOR e CANDIDATO para fazer uma análise mais precisa das respostas do candidato.

1. **Experiência Profissional e Conquistas:**
   - Resuma a trajetória profissional do candidato, incluindo cargos principais, empresas e progressão na carreira
   - Destaque as conquistas, projetos ou realizações mais significativas mencionadas pelo CANDIDATO
   - Anote quaisquer certificações, educação ou habilidades técnicas relevantes discutidas
   - Inclua exemplos específicos que o candidato forneceu

2. **Histórico Pessoal:**
   - Descreva o histórico pessoal relevante, motivações ou experiências de vida que o CANDIDATO compartilhou
   - Inclua interesses pessoais, valores ou circunstâncias que possam ser relevantes para seu perfil profissional
   - Anote quaisquer desafios superados ou perspectivas únicas mencionadas pelo candidato

3. **Avaliação de Personalidade e Habilidades:**
   - Com base na linguagem, tom e exemplos fornecidos pelo CANDIDATO, identifique os principais traços de personalidade
   - Avalie habilidades interpessoais como estilo de comunicação, abordagem de resolução de problemas, capacidades de trabalho em equipe
   - Anote qualidades de liderança, adaptabilidade e outros indicadores comportamentais observados nas respostas
   - Inclua exemplos específicos da transcrição (citando falas do CANDIDATO) que apoiem essas avaliações
   - Observe como o candidato responde às perguntas do ENTREVISTADOR

4. **Pontos Fortes e Áreas de Desenvolvimento:**
   - Liste os principais pontos fortes identificados com base na entrevista
   - Identifique possíveis áreas de desenvolvimento ou lacunas mencionadas
   - Forneça recomendações específicas baseadas no perfil apresentado

5. **Adequação Cultural e Motivacional:**
   - Avalie a motivação do candidato para a posição/empresa
   - Identifique valores e características que podem indicar boa adequação cultural
   - Analise expectativas de carreira e alinhamento com objetivos organizacionais

Certifique-se de que todas as informações vêm diretamente da transcrição e evite fazer suposições além do que foi explicitamente discutido pelo CANDIDATO.
"""

# --------------------------------------------------
# Arquivo: ./app/services/analysis.py
# --------------------------------------------------
from typing import Optional
import logging
from app.infrastructure.ai.gemini import GeminiService
from app.prompts.interview_analysis import INTERVIEW_ANALYSIS_PROMPT
from app.core.exceptions import AnalysisError

logger = logging.getLogger(__name__)


class AnalysisService:
    def __init__(self):
        self.gemini = GeminiService()
    
    async def generate_report(self, transcript: str) -> Optional[str]:
        """Generate comprehensive interview analysis"""
        try:
            if not transcript or len(transcript.strip()) < 100:
                raise AnalysisError("Transcript too short for analysis")
            
            logger.info("Generating interview analysis", extra={
                "transcript_length": len(transcript)
            })
            
            analysis = await self.gemini.generate_analysis(
                transcript=transcript,
                prompt=INTERVIEW_ANALYSIS_PROMPT
            )
            
            if analysis:
                logger.info("Analysis generated successfully", extra={
                    "analysis_length": len(analysis)
                })
            
            return analysis
            
        except Exception as e:
            logger.error("Analysis generation failed", extra={
                "error": str(e)
            })
            raise AnalysisError(f"Failed to generate analysis: {str(e)}")

# --------------------------------------------------
# Arquivo: ./app/services/audio_processor.py
# --------------------------------------------------
from typing import List, Tuple
import io
import logging
from pydub import AudioSegment
from app.core.exceptions import AudioProcessingError

logger = logging.getLogger(__name__)


class AudioProcessor:
    def __init__(self, chunk_duration_minutes: int = 15):
        self.chunk_duration_minutes = chunk_duration_minutes
    
    def convert_to_mp3(self, audio_bytes: bytes) -> bytes:
        """Convert any audio format to MP3"""
        try:
            audio = AudioSegment.from_file(io.BytesIO(audio_bytes))
            
            mp3_buffer = io.BytesIO()
            audio.export(mp3_buffer, format="mp3")
            mp3_bytes = mp3_buffer.getvalue()
            
            logger.info("Audio converted to MP3", extra={
                "original_size": len(audio_bytes),
                "mp3_size": len(mp3_bytes)
            })
            
            return mp3_bytes
            
        except Exception as e:
            logger.error("Audio conversion failed", extra={
                "error": str(e)
            })
            raise AudioProcessingError(f"Failed to convert audio: {str(e)}")
    
    def split_into_chunks(self, audio_bytes: bytes) -> List[Tuple[bytes, float, float]]:
        """Split audio into chunks. Returns (chunk_bytes, start_minutes, duration_minutes)"""
        try:
            audio = AudioSegment.from_file(io.BytesIO(audio_bytes))
            
            chunk_duration_ms = self.chunk_duration_minutes * 60 * 1000
            total_duration_ms = len(audio)
            
            chunks = []
            start_time_ms = 0
            
            logger.info("Splitting audio", extra={
                "total_duration_minutes": total_duration_ms / 1000 / 60,
                "chunk_duration_minutes": self.chunk_duration_minutes
            })
            
            while start_time_ms < total_duration_ms:
                end_time_ms = min(start_time_ms + chunk_duration_ms, total_duration_ms)
                
                chunk = audio[start_time_ms:end_time_ms]
                
                chunk_buffer = io.BytesIO()
                chunk.export(chunk_buffer, format="mp3", parameters=["-q:a", "5"])
                chunk_bytes = chunk_buffer.getvalue()
                
                start_time_minutes = start_time_ms / 1000 / 60
                actual_duration_minutes = (end_time_ms - start_time_ms) / 1000 / 60
                
                chunks.append((chunk_bytes, start_time_minutes, actual_duration_minutes))
                
                logger.info("Chunk created", extra={
                    "chunk_index": len(chunks),
                    "start_minutes": start_time_minutes,
                    "duration_minutes": actual_duration_minutes,
                    "size_bytes": len(chunk_bytes)
                })
                
                start_time_ms = end_time_ms
            
            return chunks
            
        except Exception as e:
            logger.error("Audio splitting failed", extra={
                "error": str(e)
            })
            raise AudioProcessingError(f"Failed to split audio: {str(e)}")

# --------------------------------------------------
# Arquivo: ./app/services/document_generator.py
# --------------------------------------------------
import os
import tempfile
from datetime import datetime
from typing import Tuple
from docx import Document
from docx.shared import Inches
import logging

logger = logging.getLogger(__name__)


class DocumentGenerator:
    def create_documents(
        self, 
        transcript: str, 
        analysis: str, 
        identifier: str
    ) -> Tuple[str, str]:
        """Create transcript and analysis documents"""
        transcript_path = self._create_transcript_document(transcript, identifier)
        analysis_path = self._create_analysis_document(analysis, identifier)
        
        return transcript_path, analysis_path
    
    def _create_transcript_document(self, transcript: str, identifier: str) -> str:
        """Create Word document with transcript"""
        try:
            doc = Document()
            
            # Title
            title = doc.add_heading('Transcrição da Entrevista', 0)
            title.alignment = 1
            
            # Metadata
            doc.add_paragraph(f"Gerado em: {datetime.now().strftime('%d de %B de %Y às %H:%M')}")
            doc.add_paragraph(f"ID: {identifier}")
            doc.add_paragraph("")
            
            # Content
            doc.add_paragraph("Transcrição completa com timestamps:")
            doc.add_paragraph("")
            
            # Add transcript with formatting
            for line in transcript.split('\n'):
                if line.strip():
                    if line.startswith(('ENTREVISTADOR:', 'CANDIDATO:', 'LOCUTOR')):
                        para = doc.add_paragraph()
                        speaker_end = line.find(':')
                        if speaker_end > 0:
                            run = para.add_run(line[:speaker_end + 1])
                            run.bold = True
                            para.add_run(line[speaker_end + 1:])
                        else:
                            para.add_run(line)
                    else:
                        doc.add_paragraph(line)
                else:
                    doc.add_paragraph("")
            
            # Save
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"transcricao_{identifier}_{timestamp}.docx"
            doc_path = os.path.join(tempfile.gettempdir(), filename)
            doc.save(doc_path)
            
            logger.info("Transcript document created", extra={
                "file_path": doc_path
            })
            
            return doc_path
            
        except Exception as e:
            logger.error("Failed to create transcript document", extra={
                "error": str(e)
            })
            raise
    
    def _create_analysis_document(self, analysis: str, identifier: str) -> str:
        """Create Word document with analysis"""
        try:
            doc = Document()
            
            # Title
            title = doc.add_heading('Análise da Entrevista', 0)
            title.alignment = 1
            
            # Metadata
            doc.add_paragraph(f"Gerado em: {datetime.now().strftime('%d de %B de %Y às %H:%M')}")
            doc.add_paragraph(f"ID: {identifier}")
            doc.add_paragraph("")
            
            # Content
            doc.add_paragraph("Análise estruturada baseada na transcrição:")
            doc.add_paragraph("")
            
            # Parse and format analysis
            sections = analysis.split('**')
            current_text = ""
            
            for i, section in enumerate(sections):
                if i % 2 == 1:  # Heading
                    if current_text.strip():
                        doc.add_paragraph(current_text.strip())
                        current_text = ""
                    doc.add_heading(section.strip(), level=1)
                else:
                    current_text += section
            
            if current_text.strip():
                doc.add_paragraph(current_text.strip())
            
            # Save
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"analise_{identifier}_{timestamp}.docx"
            doc_path = os.path.join(tempfile.gettempdir(), filename)
            doc.save(doc_path)
            
            logger.info("Analysis document created", extra={
                "file_path": doc_path
            })
            
            return doc_path
            
        except Exception as e:
            logger.error("Failed to create analysis document", extra={
                "error": str(e)
            })
            raise

# --------------------------------------------------
# Arquivo: ./app/services/message_handler.py
# --------------------------------------------------
from typing import Dict
import logging
import os
from app.domain.entities.interview import Interview, InterviewStatus
from app.infrastructure.database.repositories.interview import InterviewRepository
from app.infrastructure.whatsapp.client import WhatsAppClient
from app.services.audio_processor import AudioProcessor
from app.services.transcription import TranscriptionService
from app.services.analysis import AnalysisService
from app.services.document_generator import DocumentGenerator
from app.core.config import settings

logger = logging.getLogger(__name__)


class MessageHandler:
    def __init__(self):
        self.interview_repo = InterviewRepository()
        self.whatsapp = WhatsAppClient()
        self.audio_processor = AudioProcessor(settings.AUDIO_CHUNK_MINUTES)
        self.transcription = TranscriptionService()
        self.analysis = AnalysisService()
        self.doc_generator = DocumentGenerator()
    
    async def process_audio_message(self, message_data: Dict):
        """Process audio message with full error handling"""
        interview = None
        
        try:
            # Create interview record
            interview = Interview(
                phone_number=message_data["from"],
                message_id=message_data["message_id"],
                audio_id=message_data["media_id"],
                simple_mode=message_data.get("simple_mode", False)
            )
            
            await self.interview_repo.create(interview)
            
            # Update status
            interview.mark_processing()
            await self.interview_repo.update(interview)
            
            # Process audio
            await self._process_audio(interview)
            
        except Exception as e:
            logger.error("Audio processing failed", extra={
                "error": str(e),
                "interview_id": interview.id if interview else "unknown"
            })
            
            if interview:
                interview.mark_failed(str(e))
                await self.interview_repo.update(interview)
                
                await self.whatsapp.send_text_message(
                    interview.phone_number,
                    f"❌ Erro no processamento: {str(e)}"
                )
    
    async def _process_audio(self, interview: Interview):
        """Internal audio processing logic"""
        # Step 1: Download audio
        await self.whatsapp.send_text_message(
            interview.phone_number,
            "🎵 Baixando áudio..."
        )
        
        audio_bytes = await self.whatsapp.download_media(interview.audio_id)
        if not audio_bytes:
            raise Exception("Failed to download audio")
        
        interview.audio_size_mb = len(audio_bytes) / (1024 * 1024)
        
        # Step 2: Convert and split
        mode_text = "SIMPLES" if interview.simple_mode else "COMPLETO"
        await self.whatsapp.send_text_message(
            interview.phone_number,
            f"🔄 Convertendo e dividindo áudio ({interview.audio_size_mb:.1f}MB)\n📝 Modo: {mode_text}"
        )
        
        mp3_bytes = self.audio_processor.convert_to_mp3(audio_bytes)
        chunks = self.audio_processor.split_into_chunks(mp3_bytes)
        
        interview.chunks_total = len(chunks)
        await self.interview_repo.update(interview)
        
        # Step 3: Transcribe
        interview.status = InterviewStatus.TRANSCRIBING
        await self.interview_repo.update(interview)
        
        transcript = await self.transcription.transcribe_chunks(
            chunks, interview, self._update_progress
        )
        
        if not transcript:
            raise Exception("Transcription failed")
        
        interview.transcript = transcript
        
        # Step 4: Generate analysis
        interview.status = InterviewStatus.ANALYZING
        await self.interview_repo.update(interview)
        
        await self.whatsapp.send_text_message(
            interview.phone_number,
            "🧠 Gerando análise estruturada..."
        )
        
        analysis = await self.analysis.generate_report(transcript)
        if analysis:
            interview.analysis = analysis
        
        # Step 5: Create and send documents
        await self._create_and_send_documents(interview)
        
        # Mark completed
        interview.mark_completed()
        await self.interview_repo.update(interview)
        
        # Final message
        await self.whatsapp.send_text_message(
            interview.phone_number,
            f"🎉 Processamento completo! (ID: {interview.id[:8]})\n\n"
            f"📝 Modo: {mode_text}\n"
            f"📄 {2 if analysis else 1} documento(s) enviado(s)\n"
            f"⏱️ Processamento em background concluído!"
        )
    
    async def _update_progress(self, interview: Interview, chunk_num: int):
        """Update processing progress"""
        interview.chunks_processed = chunk_num
        await self.interview_repo.update(interview)
        
        mode_text = "SIMPLES" if interview.simple_mode else "COMPLETO"
        await self.whatsapp.send_text_message(
            interview.phone_number,
            f"🎙️ Transcrevendo chunk {chunk_num}/{interview.chunks_total} - Modo {mode_text}"
        )
    
    async def _create_and_send_documents(self, interview: Interview):
        """Create and send documents"""
        await self.whatsapp.send_text_message(
            interview.phone_number,
            "📄 Criando documentos..."
        )
        
        # Create transcript document
        transcript_path, analysis_path = self.doc_generator.create_documents(
            interview.transcript,
            interview.analysis or "Análise não disponível",
            interview.id
        )
        
        try:
            # Upload and send transcript
            transcript_media_id = await self.whatsapp.upload_media(transcript_path)
            if transcript_media_id:
                await self.whatsapp.send_document(
                    interview.phone_number,
                    transcript_media_id,
                    f"📝 TRANSCRIÇÃO (ID: {interview.id[:8]})",
                    f"transcricao_{interview.id[:8]}.docx"
                )
            
            # Upload and send analysis if available
            if interview.analysis and analysis_path:
                analysis_media_id = await self.whatsapp.upload_media(analysis_path)
                if analysis_media_id:
                    await self.whatsapp.send_document(
                        interview.phone_number,
                        analysis_media_id,
                        f"📊 ANÁLISE (ID: {interview.id[:8]})",
                        f"analise_{interview.id[:8]}.docx"
                    )
            
        finally:
            # Clean up files
            for path in [transcript_path, analysis_path]:
                try:
                    if path and os.path.exists(path):
                        os.remove(path)
                except:
                    pass

# --------------------------------------------------
# Arquivo: ./app/services/transcription.py
# --------------------------------------------------
from typing import Optional, List, Tuple, Callable
import logging
from app.infrastructure.ai.whisper import WhisperService
from app.domain.entities.interview import Interview
from app.core.exceptions import TranscriptionError

logger = logging.getLogger(__name__)


class TranscriptionService:
    def __init__(self):
        self.whisper = WhisperService()
    
    async def transcribe_chunks(
        self,
        chunks: List[Tuple[bytes, float, float]],
        interview: Interview,
        progress_callback: Optional[Callable] = None
    ) -> Optional[str]:
        """Transcribe audio chunks with progress tracking"""
        try:
            full_transcript = ""
            
            for i, (chunk_bytes, start_time_minutes, duration_minutes) in enumerate(chunks):
                logger.info("Transcribing chunk", extra={
                    "chunk_index": i + 1,
                    "total_chunks": len(chunks),
                    "start_time_minutes": start_time_minutes,
                    "duration_minutes": duration_minutes
                })
                
                # Progress callback
                if progress_callback:
                    await progress_callback(interview, i + 1)
                
                # Transcribe chunk
                if interview.simple_mode:
                    chunk_transcript = await self._transcribe_simple(chunk_bytes)
                else:
                    chunk_transcript = await self._transcribe_with_speakers(chunk_bytes)
                
                if not chunk_transcript:
                    logger.warning("Chunk transcription failed", extra={
                        "chunk_index": i + 1
                    })
                    continue
                
                # Adjust timestamps if not first chunk
                if start_time_minutes > 0:
                    chunk_transcript = self._adjust_timestamps(
                        chunk_transcript, 
                        start_time_minutes
                    )
                
                # Combine transcripts
                if full_transcript:
                    full_transcript += "\n\n" + chunk_transcript
                else:
                    full_transcript = chunk_transcript
            
            return full_transcript if full_transcript else None
            
        except Exception as e:
            logger.error("Chunk transcription process failed", extra={
                "error": str(e),
                "interview_id": interview.id
            })
            raise TranscriptionError(f"Failed to transcribe chunks: {str(e)}")
    
    async def _transcribe_with_speakers(self, audio_bytes: bytes) -> Optional[str]:
        """Transcribe with speaker identification using Whisper + post-processing"""
        try:
            result = await self.whisper.transcribe(audio_bytes)
            
            if not result or not result.get("text"):
                return None
            
            # Convert Whisper segments to speaker-identified transcript
            # This is a simplified approach - in production you might want
            # to use additional speaker diarization models
            transcript_lines = []
            current_speaker = "LOCUTOR 1"
            
            for segment in result.get("segments", []):
                start_min = int(segment["start"] // 60)
                start_sec = int(segment["start"] % 60)
                end_min = int(segment["end"] // 60)
                end_sec = int(segment["end"] % 60)
                
                timestamp = f"[{start_min:02d}:{start_sec:02d}-{end_min:02d}:{end_sec:02d}]"
                text = segment["text"].strip()
                
                # Simple speaker alternation logic
                # In production, use proper speaker diarization
                if len(transcript_lines) > 0 and len(text) > 50:
                    current_speaker = "LOCUTOR 2" if current_speaker == "LOCUTOR 1" else "LOCUTOR 1"
                
                transcript_lines.append(f"{current_speaker}: {timestamp} {text}")
            
            return "\n".join(transcript_lines)
            
        except Exception as e:
            logger.error("Speaker transcription failed", extra={
                "error": str(e)
            })
            return None
    
    async def _transcribe_simple(self, audio_bytes: bytes) -> Optional[str]:
        """Simple transcription without speaker identification"""
        try:
            result = await self.whisper.transcribe(audio_bytes)
            
            if not result or not result.get("text"):
                return None
            
            # Convert to simple timestamped format
            transcript_lines = []
            
            for segment in result.get("segments", []):
                start_min = int(segment["start"] // 60)
                start_sec = int(segment["start"] % 60)
                end_min = int(segment["end"] // 60)
                end_sec = int(segment["end"] % 60)
                
                timestamp = f"[{start_min:02d}:{start_sec:02d}-{end_min:02d}:{end_sec:02d}]"
                text = segment["text"].strip()
                
                transcript_lines.append(f"{timestamp} {text}")
            
            return "\n".join(transcript_lines)
            
        except Exception as e:
            logger.error("Simple transcription failed", extra={
                "error": str(e)
            })
            return None
    
    def _adjust_timestamps(self, transcript: str, offset_minutes: float) -> str:
        """Adjust timestamps by adding offset"""
        import re
        
        def adjust_match(match):
            start_min = int(match.group(1)) + int(offset_minutes)
            start_sec = int(match.group(2))
            
            if match.group(3) and match.group(4):  # Range format
                end_min = int(match.group(3)) + int(offset_minutes)
                end_sec = int(match.group(4))
                return f"[{start_min:02d}:{start_sec:02d}-{end_min:02d}:{end_sec:02d}]"
            else:  # Single timestamp
                return f"[{start_min:02d}:{start_sec:02d}]"
        
        pattern = r'\[(\d{1,2}):(\d{2})(?:-(\d{1,2}):(\d{2}))?\]'
        return re.sub(pattern, adjust_match, transcript)

# --------------------------------------------------
# Arquivo: ./docker-compose.yml
# --------------------------------------------------
version: '3.8'

services:
  interview-bot:
    build:
      context: .
      dockerfile: docker/Dockerfile
    ports:
      - "8000:8000"
    environment:
      - ENVIRONMENT=production
      - DEBUG=false
    env_file:
      - .env
    volumes:
      - ./logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.interview-bot.rule=Host(\`your-domain.com\`)"
      - "traefik.http.routers.interview-bot.tls.certresolver=letsencrypt"

  # Optional: Add monitoring
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    profiles:
      - monitoring

volumes:
  prometheus_data:

# --------------------------------------------------
# Arquivo: ./docker/Dockerfile
# --------------------------------------------------
FROM python:3.11-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    ffmpeg \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY app/ ./app/

# Create non-root user
RUN useradd --create-home --shell /bin/bash app && chown -R app:app /app
USER app

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health/live || exit 1

# Run application
CMD ["gunicorn", "app.main:app", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", "-b", "0.0.0.0:8000"]

# --------------------------------------------------
# Arquivo: ./fly.toml
# --------------------------------------------------
# fly.toml app configuration file
app = "whatsappbots"
primary_region = "gru"

[build]

[http_service]
  internal_port = 8000
  force_https = true
  auto_stop_machines = "off"  # CRITICAL: Don't auto-stop during long processing!
  auto_start_machines = true
  min_machines_running = 1    # Keep at least 1 machine running
  processes = ["app"]

# Configure machine resources - increased for long audio processing
[[vm]]
  memory = "2gb"  # 2GB RAM for 1+ hour audio files
  cpu_kind = "shared"
  cpus = 1
# --------------------------------------------------
# Arquivo: ./gerar.sh
# --------------------------------------------------
#!/bin/bash

# --- Configuração ---
# Diretório raiz do projeto (padrão: diretório atual)
PROJECT_ROOT="."

# Nome do arquivo de saída
OUTPUT_FILE="project_context.txt"

# Diretórios a serem completamente ignorados na árvore e na busca de arquivos.
# Adicione outros se necessário, como 'node_modules'.
EXCLUDE_DIRS=(
    '__pycache__'
    'venv'
    '.venv'
    'env'
    '.env'
    '.git'
    '.vscode'
    '.idea'
    'dist'
    'build'
    '*.egg-info'
    '.pytest_cache'
    'htmlcov'
    '.ipynb_checkpoints'
)

# Padrões de nome de arquivo a serem incluídos na seção de conteúdo.
# Sinta-se à vontade para adicionar ou remover tipos de arquivo (ex: '*.html', '*.css').
INCLUDE_PATTERNS=(
    -name '*.py'
    -o -name '*.pyi'
    -o -name 'requirements*.txt'
    -o -name 'README.md'
    -o -name 'pyproject.toml'
    -o -name 'Pipfile'
    -o -name 'Pipfile.lock'
    -o -name '*.ini'
    -o -name '*.toml'
    -o -name '*.yaml'
    -o -name '*.yml'
    -o -name '*.json'
    -o -name 'Dockerfile'
    -o -name 'docker-compose.yml'
    -o -name '*.sh'
)
# --- Fim da Configuração ---


echo "Gerando contexto do projeto em '$OUTPUT_FILE'..."

# Remove o arquivo de saída antigo, se existir
rm -f "$OUTPUT_FILE"

# --- Seção 1: Estrutura de Diretórios ---
echo "==================================================" > "$OUTPUT_FILE"
echo "           ESTRUTURA DO PROJETO (tree -L 3)       " >> "$OUTPUT_FILE"
echo "==================================================" >> "$OUTPUT_FILE"
echo "" >> "$OUTPUT_FILE"

# Constrói o padrão de exclusão para o comando 'tree'
TREE_IGNORE_PATTERN=$(IFS="|"; echo "${EXCLUDE_DIRS[*]}")
tree -L 3 -a -I "$TREE_IGNORE_PATTERN" "$PROJECT_ROOT" >> "$OUTPUT_FILE"

# --- Seção 2: Conteúdo dos Arquivos ---
echo "" >> "$OUTPUT_FILE"
echo "" >> "$OUTPUT_FILE"
echo "==================================================" >> "$OUTPUT_FILE"
echo "       CÓDIGO FONTE E ARQUIVOS DE CONFIGURAÇÃO    " >> "$OUTPUT_FILE"
echo "==================================================" >> "$OUTPUT_FILE"

# Constrói a condição de exclusão para o comando 'find'
PRUNE_CONDITIONS=()
for dir in "${EXCLUDE_DIRS[@]}"; do
    PRUNE_CONDITIONS+=(-o -path "*/$dir")
done
# Remove o '-o' inicial desnecessário
unset PRUNE_CONDITIONS[0]

# Encontra e concatena os arquivos relevantes, ignorando os diretórios excluídos
find "$PROJECT_ROOT" \( "${PRUNE_CONDITIONS[@]}" \) -prune -o -type f \( "${INCLUDE_PATTERNS[@]}" \) -print | sort | while IFS= read -r file; do
    # Verifica se o arquivo não está vazio
    if [ -s "$file" ]; then
        echo "" >> "$OUTPUT_FILE"
        echo "# --------------------------------------------------" >> "$OUTPUT_FILE"
        echo "# Arquivo: $file" >> "$OUTPUT_FILE"
        echo "# --------------------------------------------------" >> "$OUTPUT_FILE"
        cat "$file" >> "$OUTPUT_FILE"
    fi
done

echo ""
echo "✅ Processo concluído!"
echo "O contexto do projeto foi salvo em: $OUTPUT_FILE"
echo ""
echo "⚠️ IMPORTANTE: Revise o arquivo '$OUTPUT_FILE' antes de compartilhá-lo para garantir que nenhuma informação sensível (como senhas ou chaves de API) foi incluída."
# --------------------------------------------------
# Arquivo: ./requirements.txt
# --------------------------------------------------
aiohttp==3.9.1
aiosignal==1.3.2
annotated-types==0.7.0
anyio==3.7.1
attrs==25.3.0
black==23.11.0
cachetools==5.5.2
certifi==2025.6.15
charset-normalizer==3.4.2
click==8.2.1
distro==1.9.0
dnspython==2.7.0
fastapi==0.115.14
frozenlist==1.7.0
google-ai-generativelanguage==0.6.15
google-api-core==2.25.1
google-api-python-client==2.174.0
google-auth==2.40.3
google-auth-httplib2==0.2.0
google-generativeai==0.8.5
googleapis-common-protos==1.70.0
grpcio==1.73.1
grpcio-status==1.62.3
gunicorn==21.2.0
h11==0.16.0
httpcore==1.0.9
httplib2==0.22.0
httptools==0.6.4
httpx==0.28.1
idna==3.10
iniconfig==2.1.0
lxml==6.0.0
motor==3.3.2
multidict==6.6.0
mypy==1.7.1
mypy_extensions==1.1.0
openai==1.3.8
packaging==25.0
pathspec==0.12.1
phonenumbers==8.13.26
platformdirs==4.3.8
pluggy==1.6.0
propcache==0.3.2
proto-plus==1.26.1
protobuf==4.25.8
pyasn1==0.6.1
pyasn1_modules==0.4.2
pydantic==2.11.7
pydantic-settings==2.1.0
pydantic_core==2.33.2
pydub==0.25.1
pymongo==4.6.0
pyparsing==3.2.3
pytest==7.4.3
pytest-asyncio==0.21.1
python-docx==1.1.0
python-dotenv==1.1.1
PyYAML==6.0.2
requests==2.31.0
rsa==4.9.1
sniffio==1.3.1
starlette==0.46.2
tqdm==4.67.1
typing-inspection==0.4.1
typing_extensions==4.14.0
uritemplate==4.2.0
urllib3==2.5.0
uvicorn==0.24.0
uvloop==0.21.0
watchfiles==1.1.0
websockets==15.0.1
yarl==1.20.1

# --------------------------------------------------
# Arquivo: ./scripts/run.sh
# --------------------------------------------------
#!/bin/bash

# Run script for Interview Bot

set -e

echo "🚀 Starting Interview Bot..."

# Activate virtual environment
source .venv/bin/activate

# Run with auto-reload in development
if [ "$1" = "dev" ]; then
    echo "🔄 Running in development mode with auto-reload..."
    uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
else
    echo "🏃 Running in production mode..."
    gunicorn app.main:app -w 4 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8000
fi

# --------------------------------------------------
# Arquivo: ./scripts/setup.sh
# --------------------------------------------------
#!/bin/bash

# Setup script for Interview Bot

set -e

echo "🚀 Setting up Interview Bot..."

# Create virtual environment
echo "📦 Creating virtual environment..."
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
echo "📥 Installing dependencies..."
pip install --upgrade pip
pip install -r requirements.txt

# Copy environment file
if [ ! -f .env ]; then
    echo "📋 Creating .env file..."
    cp .env.example .env
    echo "⚠️  Please edit .env file with your actual credentials!"
fi

# Create logs directory
mkdir -p logs

echo "✅ Setup complete!"
echo ""
echo "Next steps:"
echo "1. Edit .env file with your credentials"
echo "2. Set up MongoDB Atlas cluster"
echo "3. Run: source venv/bin/activate"
echo "4. Run: python -m app.main"

# --------------------------------------------------
# Arquivo: ./scripts/test.sh
# --------------------------------------------------
#!/bin/bash

# Test script for Interview Bot

set -e

echo "🧪 Running tests..."

# Activate virtual environment
source venv/bin/activate

# Run tests
pytest tests/ -v --tb=short

# Run linting
echo "🔍 Running linting..."
black --check app/
mypy app/

echo "✅ All tests passed!"

# --------------------------------------------------
# Arquivo: ./tests/test_main.py
# --------------------------------------------------
import pytest
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)


def test_root():
    response = client.get("/")
    assert response.status_code == 200
    data = response.json()
    assert "Interview Bot" in data["message"]


def test_health_live():
    response = client.get("/health/live")
    assert response.status_code == 200
    data = response.json()
    assert data["status"] == "alive"


def test_webhook_verification():
    response = client.get("/webhook", params={
        "hub.mode": "subscribe",
        "hub.verify_token": "test_token",
        "hub.challenge": "test_challenge"
    })
    # Will fail without proper token, but tests the endpoint
    assert response.status_code in [200, 403]

# --------------------------------------------------
# Arquivo: ./tests/unit/test_phone_number.py
# --------------------------------------------------
import pytest
from app.domain.value_objects.phone_number import BrazilianPhoneNumber


def test_valid_brazilian_number():
    phone = BrazilianPhoneNumber(number="5511999887766")
    assert phone.number == "5511999887766"


def test_fix_missing_ninth_digit():
    phone = BrazilianPhoneNumber(number="551199887766")
    assert phone.number == "5511999887766"


def test_invalid_number():
    with pytest.raises(ValueError):
        BrazilianPhoneNumber(number="1234567890")
